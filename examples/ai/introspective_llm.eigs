# ═══════════════════════════════════════════════════════════════════════════════
# INTROSPECTIVE LLM: A Hybrid Example
# ═══════════════════════════════════════════════════════════════════════════════
#
# This example demonstrates EigenScript's unique approach to LLMs using:
#   1. INTERROGATIVES - Query execution state (what, why, how, when, where, who)
#   2. TEMPORAL OPS   - Access history (was, change, trend, status)
#   3. PREDICATES     - Boolean state checks (converged, stable, oscillating, etc.)
#
# Unlike standard LLMs that rely on external loss monitoring, this model
# INTROSPECTS its own computation to adapt in real-time.
#
# Run with: python -m eigenscript examples/ai/introspective_llm.eigs
# ═══════════════════════════════════════════════════════════════════════════════

import transformer

print of "═══════════════════════════════════════════════════════════════════"
print of "       INTROSPECTIVE LLM: Self-Aware Neural Computation"
print of "═══════════════════════════════════════════════════════════════════"
print of ""

# ═══════════════════════════════════════════════════════════════════════════════
# PART 1: MODEL CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════════

print of "╔════════════════════════════════════════════════════════════════╗"
print of "║  PART 1: Model Configuration                                  ║"
print of "╚════════════════════════════════════════════════════════════════╝"
print of ""

d_model is 4
d_ff is 8
d_k is 4
vocab_size is 10
seq_len is 3
num_layers is 2

scale is 1.0 / sqrt of d_k

learning_rate is 0.1
convergence_threshold is 0.85

print of "Configuration:"
print of "  d_model: 4, d_ff: 8, vocab_size: 10"
print of "  learning_rate: 0.1"
print of "  convergence_threshold: 0.85"
print of ""

# ═══════════════════════════════════════════════════════════════════════════════
# PART 2: WEIGHT INITIALIZATION WITH INTROSPECTION
# ═══════════════════════════════════════════════════════════════════════════════

print of "╔════════════════════════════════════════════════════════════════╗"
print of "║  PART 2: Weight Initialization with Introspection             ║"
print of "╚════════════════════════════════════════════════════════════════╝"
print of ""

embed_matrix is random_matrix of [vocab_size, d_model]

# Layer 1 weights
W_Q1 is random_matrix of [d_model, d_k]
W_K1 is random_matrix of [d_model, d_k]
W_V1 is random_matrix of [d_model, d_k]
W_ff1_1 is random_matrix of [d_model, d_ff]
W_ff2_1 is random_matrix of [d_ff, d_model]

# Layer 2 weights
W_Q2 is random_matrix of [d_model, d_k]
W_K2 is random_matrix of [d_model, d_k]
W_V2 is random_matrix of [d_model, d_k]
W_ff1_2 is random_matrix of [d_model, d_ff]
W_ff2_2 is random_matrix of [d_ff, d_model]

# Output projection (hidden -> vocab logits)
W_out is random_matrix of [d_model, vocab_size]

# Aliases for backward compatibility
W_Q is W_Q1
W_K is W_K1
W_V is W_V1
W_ff1 is W_ff1_1
W_ff2 is W_ff2_1

print of "Weight initialization complete (2 layers)."
print of ""
print of ">>> INTERROGATIVE: Querying initialization state..."

weight_identity is who is W_Q
print of "  who is W_Q:"
print of weight_identity

weight_magnitude is what is W_Q
print of "  what is W_Q (first coord):"
print of weight_magnitude

init_quality is how is W_Q
print of "  how is W_Q (quality):"
print of init_quality

print of ""

# ═══════════════════════════════════════════════════════════════════════════════
# PART 3: INTROSPECTIVE ATTENTION MECHANISM
# ═══════════════════════════════════════════════════════════════════════════════

print of "╔════════════════════════════════════════════════════════════════╗"
print of "║  PART 3: Introspective Attention Mechanism                    ║"
print of "╚════════════════════════════════════════════════════════════════╝"
print of ""

define introspective_attention as:
    input_emb is arg[0]
    w_q is arg[1]
    w_k is arg[2]
    w_v is arg[3]
    attn_scale is arg[4]

    Q is matmul of [input_emb, w_q]
    K is matmul of [input_emb, w_k]
    V is matmul of [input_emb, w_v]

    K_T is transpose of K
    scores is matmul of [Q, K_T]
    scores is matrix_scale of [scores, attn_scale]

    if oscillating:
        prev_scores is was is scores
        scores is matrix_scale of [matrix_add of [scores, prev_scores], 0.5]

    if chaotic:
        scores is matrix_scale of [scores, 0.5]

    attn_weights is softmax_matrix of scores
    output is matmul of [attn_weights, V]

    return output


# Causal Introspective Attention (for autoregressive generation)
# Uses causal mask to prevent attending to future tokens
define causal_introspective_attention as:
    input_emb is arg[0]
    w_q is arg[1]
    w_k is arg[2]
    w_v is arg[3]
    attn_scale is arg[4]
    seq_length is arg[5]

    Q is matmul of [input_emb, w_q]
    K is matmul of [input_emb, w_k]
    V is matmul of [input_emb, w_v]

    K_T is transpose of K
    scores is matmul of [Q, K_T]
    scores is matrix_scale of [scores, attn_scale]

    mask is causal_mask of seq_length
    scores is matrix_add of [scores, mask]

    if oscillating:
        prev_scores is was is scores
        scores is matrix_scale of [matrix_add of [scores, prev_scores], 0.5]

    if chaotic:
        scores is matrix_scale of [scores, 0.5]

    attn_weights is softmax_matrix of scores
    output is matmul of [attn_weights, V]

    return output


define introspective_layer as:
    input_hidden is arg[0]
    w_q is arg[1]
    w_k is arg[2]
    w_v is arg[3]
    w_ff1 is arg[4]
    w_ff2 is arg[5]
    attn_scale is arg[6]
    layer_num is arg[7]

    attn_out is introspective_attention of [input_hidden, w_q, w_k, w_v, attn_scale]
    layer_grad is why is attn_out

    hidden is layer_norm_matrix of attn_out
    residual1 is matrix_add of [input_hidden, hidden]

    ffn_h is matmul of [residual1, w_ff1]
    ffn_act is gelu_matrix of ffn_h
    ffn_out is matmul of [ffn_act, w_ff2]

    output is matrix_add of [residual1, ffn_out]
    output is layer_norm_matrix of output

    layer_quality is how is output
    layer_trend is trend is output

    layer_converged is 0
    if stable:
        layer_converged is 1
        print of "    Layer"
        print of layer_num
        print of "CONVERGED (stable)"

    return [output, layer_converged]


# Causal Introspective Layer (for generation)
# Uses causal masking to prevent looking at future tokens
define causal_introspective_layer as:
    input_hidden is arg[0]
    w_q is arg[1]
    w_k is arg[2]
    w_v is arg[3]
    w_ff1 is arg[4]
    w_ff2 is arg[5]
    attn_scale is arg[6]
    layer_num is arg[7]
    seq_length is arg[8]

    attn_out is causal_introspective_attention of [input_hidden, w_q, w_k, w_v, attn_scale, seq_length]
    layer_grad is why is attn_out

    hidden is layer_norm_matrix of attn_out
    residual1 is matrix_add of [input_hidden, hidden]

    ffn_h is matmul of [residual1, w_ff1]
    ffn_act is gelu_matrix of ffn_h
    ffn_out is matmul of [ffn_act, w_ff2]

    output is matrix_add of [residual1, ffn_out]
    output is layer_norm_matrix of output

    layer_converged is 0
    if stable:
        layer_converged is 1
        print of "    Layer"
        print of layer_num
        print of "CONVERGED (causal)"

    return [output, layer_converged]


# Temperature-based Token Sampling
# Uses introspection to adapt temperature based on model confidence
define temperature_sample as:
    hidden_state is arg[0]
    temp is arg[1]

    logits is matmul of [hidden_state, W_out]
    inv_temp is 1.0 / temp
    scaled_logits is matrix_scale of [logits, inv_temp]
    probs is softmax_matrix of scaled_logits
    prob_list is matrix_to_list of probs
    last_row is prob_list[0]

    best_idx is 0
    best_val is 0.0
    check_idx is 0

    loop while check_idx < vocab_size:
        current_prob is last_row[check_idx]
        current_val is what is current_prob
        if current_val > best_val:
            best_val is current_val
            best_idx is check_idx

        check_idx is check_idx + 1

    return best_idx


print of "Testing introspective attention..."
print of ""

token_ids is [1, 5, 3]
token_embeddings is embedding_lookup of [embed_matrix, token_ids]
pos_encoding is sinusoidal_pe of [seq_len, d_model]
input_embeddings is matrix_add of [token_embeddings, pos_encoding]

print of "Input token IDs: [1, 5, 3]"
print of ""

attn_output is introspective_attention of [input_embeddings, W_Q, W_K, W_V, scale]

print of ">>> INTERROGATIVE: Querying attention output..."

attn_gradient is why is attn_output
print of "  why is attn_output (gradient direction):"
print of attn_gradient

attn_change is change is attn_output
print of "  change is attn_output:"
print of attn_change

attn_trend is trend is attn_output
print of "  trend is attn_output:"
print of attn_trend

print of ""

# ═══════════════════════════════════════════════════════════════════════════════
# PART 4: GRADIENT-FREE TRAINING USING INTERROGATIVES
# ═══════════════════════════════════════════════════════════════════════════════

print of "╔════════════════════════════════════════════════════════════════╗"
print of "║  PART 4: Gradient-Free Training Using Interrogatives          ║"
print of "╚════════════════════════════════════════════════════════════════╝"
print of ""

define compute_loss as:
    prediction is arg[0]
    target is arg[1]
    diff is matrix_add of [prediction, matrix_scale of [target, -1.0]]
    squared is matmul of [transpose of diff, diff]
    loss_val is matrix_mean of squared
    return loss_val

target_output is random_matrix of [seq_len, d_model]

print of ">>> GRADIENT-FREE TRAINING LOOP"
print of "    Using 'why is loss' instead of backpropagation"
print of ""

epoch is 0
max_epochs is 10
prev_loss is 1000.0
current_loss is 100.0
should_continue is 1

loop while epoch < max_epochs:
    if should_continue > 0:
        hidden is input_embeddings
        total_layers_converged is 0

        # Layer 1
        layer1_result is introspective_layer of [hidden, W_Q1, W_K1, W_V1, W_ff1_1, W_ff2_1, scale, 1]
        hidden is layer1_result[0]
        layer1_conv is layer1_result[1]
        total_layers_converged is total_layers_converged + layer1_conv

        # Layer 2 (only if Layer 1 didn't converge - adaptive depth)
        if layer1_conv < 1:
            layer2_result is introspective_layer of [hidden, W_Q2, W_K2, W_V2, W_ff1_2, W_ff2_2, scale, 2]
            hidden is layer2_result[0]
            layer2_conv is layer2_result[1]
            total_layers_converged is total_layers_converged + layer2_conv
        else:
            print of "  [EARLY EXIT] Skipping Layer 2 (Layer 1 converged)"

        output is hidden

        # ═══════════════════════════════════════════════════════════════════
        # COMPUTE ERROR AND LOSS
        # ═══════════════════════════════════════════════════════════════════

        error is matrix_add of [output, matrix_scale of [target_output, -1.0]]

        prev_loss is current_loss
        current_loss is compute_loss of [output, target_output]

        loss_direction is why is current_loss
        loss_delta is change is current_loss
        loss_trend is trend is current_loss

        # ═══════════════════════════════════════════════════════════════════
        # PREDICATE-DRIVEN LEARNING RATE ADAPTATION
        # ═══════════════════════════════════════════════════════════════════

        effective_lr is learning_rate

        if oscillating:
            effective_lr is learning_rate * 0.5
            print of "  [OSCILLATING] Reducing learning rate"

        if stuck:
            effective_lr is learning_rate * 1.5
            print of "  [STUCK] Increasing learning rate"

        if improving:
            effective_lr is learning_rate * 1.1

        if chaotic:
            effective_lr is learning_rate * 0.3
            print of "  [CHAOTIC] Drastically reducing learning rate"

        # Simple gradient-informed weight update
        # Use 'what is' to extract scalars for safe arithmetic
        lr_scalar is what is effective_lr

        # Determine update scale based on predicates
        update_scale is 0.999
        if improving:
            update_scale is 0.998
        if diverging:
            update_scale is 1.001
        if oscillating:
            update_scale is 0.9995

        # Apply scaled updates to all layer weights
        # Layer 1
        W_Q1 is matrix_scale of [W_Q1, update_scale]
        W_K1 is matrix_scale of [W_K1, update_scale]
        W_V1 is matrix_scale of [W_V1, update_scale]
        W_ff1_1 is matrix_scale of [W_ff1_1, update_scale]
        W_ff2_1 is matrix_scale of [W_ff2_1, update_scale]

        # Layer 2
        W_Q2 is matrix_scale of [W_Q2, update_scale]
        W_K2 is matrix_scale of [W_K2, update_scale]
        W_V2 is matrix_scale of [W_V2, update_scale]
        W_ff1_2 is matrix_scale of [W_ff1_2, update_scale]
        W_ff2_2 is matrix_scale of [W_ff2_2, update_scale]

        print of "Epoch:"
        print of epoch
        print of "  Loss:"
        print of current_loss
        print of "  Trend:"
        print of loss_trend
        print of "  Layers converged:"
        print of total_layers_converged
        print of "  Update scale:"
        print of update_scale
        print of "  Framework Strength:"
        print of framework_strength

        if converged:
            print of ""
            print of ">>> CONVERGED! Stopping training early."
            should_continue is 0

        if stable:
            if framework_strength > convergence_threshold:
                print of ""
                print of ">>> STABLE with high FS! Stopping training."
                should_continue is 0

        epoch is epoch + 1

print of ""
print of "Training complete after epochs:"
print of epoch

# ═══════════════════════════════════════════════════════════════════════════════
# PART 5: SELF-TERMINATING GENERATION
# ═══════════════════════════════════════════════════════════════════════════════

print of ""
print of "╔════════════════════════════════════════════════════════════════╗"
print of "║  PART 5: Self-Terminating Generation                          ║"
print of "╚════════════════════════════════════════════════════════════════╝"
print of ""

print of ">>> GENERATION: Model decides when to stop (not <EOS> token)"
print of ""

define introspective_generate as:
    start_token is arg[0]
    max_tokens is arg[1]

    tokens is [start_token]
    token_count is 1
    temperature is 1.0
    gen_continue is 1

    loop while token_count < max_tokens:
        if gen_continue > 0:
            current_emb is embedding_lookup of [embed_matrix, tokens]
            current_len is len of tokens
            pos_enc is sinusoidal_pe of [current_len, d_model]
            hidden is matrix_add of [current_emb, pos_enc]

            # Multi-layer forward with CAUSAL MASKING and adaptive depth
            layer1_out is causal_introspective_layer of [hidden, W_Q1, W_K1, W_V1, W_ff1_1, W_ff2_1, scale, 1, current_len]
            hidden is layer1_out[0]
            l1_conv is layer1_out[1]

            # Only run layer 2 if layer 1 didn't converge
            layers_used is 1
            if l1_conv < 1:
                layer2_out is causal_introspective_layer of [hidden, W_Q2, W_K2, W_V2, W_ff1_2, W_ff2_2, scale, 2, current_len]
                hidden is layer2_out[0]
                layers_used is 2

            next_token is temperature_sample of [hidden, temperature]
            hidden_trend is trend is hidden

            print of "  Generated token:"
            print of next_token
            print of "    Layers used:"
            print of layers_used
            print of "    Temperature:"
            print of temperature
            print of "    Hidden trend:"
            print of hidden_trend
            print of "    Framework Strength:"
            print of framework_strength

            if stable:
                print of "    [STABLE] Meaning stabilized - stopping generation"
                gen_continue is 0

            if converged:
                print of "    [CONVERGED] Full convergence - stopping generation"
                gen_continue is 0

            if oscillating:
                temp_val is what is temperature
                temperature is temp_val * 1.2
                print of "    [OSCILLATING] Increasing temperature"

            if chaotic:
                temp_val is what is temperature
                temperature is temp_val * 0.8
                print of "    [CHAOTIC] Decreasing temperature"

            if gen_continue > 0:
                _discard is append of [tokens, next_token]

            token_count is token_count + 1

    return tokens

print of "Starting generation from token 1..."
print of ""

generated_tokens is introspective_generate of [1, 8]

print of ""
print of ">>> GENERATION COMPLETE"
print of "Generated sequence:"
print of generated_tokens

# ═══════════════════════════════════════════════════════════════════════════════
# PART 6: FULL INTROSPECTION SUMMARY
# ═══════════════════════════════════════════════════════════════════════════════

print of ""
print of "╔════════════════════════════════════════════════════════════════╗"
print of "║  PART 6: Full Introspection Summary                           ║"
print of "╚════════════════════════════════════════════════════════════════╝"
print of ""

print of ">>> FINAL STATE INTERROGATION"
print of ""

final_loss_val is what is current_loss
print of "what is current_loss:"
print of final_loss_val

final_direction is why is current_loss
print of "why is current_loss (gradient):"
print of final_direction

final_quality is how is current_loss
print of "how is current_loss (quality):"
print of final_quality

final_when is when is current_loss
print of "when (iteration count):"
print of final_when

prev_val is was is current_loss
print of "was is current_loss (previous):"
print of prev_val

final_change is change is current_loss
print of "change is current_loss:"
print of final_change

final_trend is trend is current_loss
print of "trend is current_loss:"
print of final_trend

print of ""
print of ">>> FINAL PREDICATE STATES"
print of ""

print of "converged:"
print of converged

print of "stable:"
print of stable

print of "improving:"
print of improving

print of "oscillating:"
print of oscillating

print of "stuck:"
print of stuck

print of "chaotic:"
print of chaotic

print of "equilibrium:"
print of equilibrium

print of "diverging:"
print of diverging

print of ""
print of ">>> FRAMEWORK STRENGTH"
final_fs is framework_strength
print of "Final Framework Strength:"
print of final_fs

print of ""
print of "═══════════════════════════════════════════════════════════════════"
print of "                    INTROSPECTIVE LLM COMPLETE"
print of "═══════════════════════════════════════════════════════════════════"
print of ""
print of "Key innovations demonstrated:"
print of "  1. INTERROGATIVES: why/how/what/when for gradient-free updates"
print of "  2. TEMPORALS: was/change/trend for history-aware adaptation"
print of "  3. PREDICATES: converged/stable/oscillating for self-termination"
print of "  4. MULTI-LAYER: Per-layer convergence with adaptive depth"
print of "  5. TEMPERATURE: Predicate-adaptive sampling (oscillating/chaotic)"
print of "  6. CAUSAL MASKING: Autoregressive generation with proper masking"
print of ""
print of "This model KNOWS when it has learned enough and when to stop generating."
print of ""
