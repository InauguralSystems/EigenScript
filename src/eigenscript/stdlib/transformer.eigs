# Transformer Implementation for EigenScript
# A complete transformer architecture using EigenScript's matrix operations

# Core Attention Mechanism
# Scaled Dot-Product Attention: attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V
define scaled_dot_product_attention as:
    query is arg[0]
    key is arg[1]
    value is arg[2]
    scale is arg[3]
    key_t is transpose of key
    scores is matmul of [query, key_t]
    scaled_scores is matrix_scale of [scores, scale]
    weights is softmax_matrix of scaled_scores
    output is matmul of [weights, value]
    return output


# Masked Attention for decoder (causal masking)
define masked_attention as:
    query is arg[0]
    key is arg[1]
    value is arg[2]
    scale is arg[3]
    mask is arg[4]
    key_t is transpose of key
    scores is matmul of [query, key_t]
    scaled_scores is matrix_scale of [scores, scale]
    masked_scores is matrix_add of [scaled_scores, mask]
    weights is softmax_matrix of masked_scores
    output is matmul of [weights, value]
    return output


# Linear projection: output = input @ weights
define linear_projection as:
    input is arg[0]
    weights is arg[1]
    return matmul of [input, weights]


# Linear with bias: output = input @ weights + bias
define linear_with_bias as:
    input is arg[0]
    weights is arg[1]
    bias is arg[2]
    projected is matmul of [input, weights]
    return matrix_add of [projected, bias]


# Single attention head computation
define single_attention_head as:
    input is arg[0]
    w_q is arg[1]
    w_k is arg[2]
    w_v is arg[3]
    scale is arg[4]
    query is matmul of [input, w_q]
    key is matmul of [input, w_k]
    value is matmul of [input, w_v]
    return scaled_dot_product_attention of [query, key, value, scale]


# Multi-head attention (simplified for current EigenScript limitations)
define multi_head_attention as:
    input is arg[0]
    w_q is arg[1]
    w_k is arg[2]
    w_v is arg[3]
    w_o is arg[4]
    num_heads is arg[5]
    d_k is arg[6]
    scale is 1.0 / sqrt of d_k
    query is matmul of [input, w_q]
    key is matmul of [input, w_k]
    value is matmul of [input, w_v]
    attended is scaled_dot_product_attention of [query, key, value, scale]
    output is matmul of [attended, w_o]
    return output


# Position-wise Feed-Forward Network with GELU
define feed_forward_network as:
    input is arg[0]
    w1 is arg[1]
    w2 is arg[2]
    hidden is matmul of [input, w1]
    activated is gelu_matrix of hidden
    output is matmul of [activated, w2]
    return output


# Position-wise FFN with ReLU
define feed_forward_relu as:
    input is arg[0]
    w1 is arg[1]
    w2 is arg[2]
    hidden is matmul of [input, w1]
    activated is relu_matrix of hidden
    output is matmul of [activated, w2]
    return output


# Simplified Encoder Layer (single-head attention)
define encoder_layer_simple as:
    input is arg[0]
    w_q is arg[1]
    w_k is arg[2]
    w_v is arg[3]
    w_ff1 is arg[4]
    w_ff2 is arg[5]
    scale is arg[6]
    norm1 is layer_norm_matrix of input
    query is matmul of [norm1, w_q]
    key is matmul of [norm1, w_k]
    value is matmul of [norm1, w_v]
    attn_out is scaled_dot_product_attention of [query, key, value, scale]
    residual1 is matrix_add of [input, attn_out]
    norm2 is layer_norm_matrix of residual1
    ffn_hidden is matmul of [norm2, w_ff1]
    ffn_activated is gelu_matrix of ffn_hidden
    ffn_out is matmul of [ffn_activated, w_ff2]
    output is matrix_add of [residual1, ffn_out]
    return output


# Create token embeddings with positional encoding
define create_embeddings as:
    token_ids is arg[0]
    embed_matrix is arg[1]
    seq_len is arg[2]
    d_model is arg[3]
    token_emb is embedding_lookup of [embed_matrix, token_ids]
    pos_enc is sinusoidal_pe of [seq_len, d_model]
    return matrix_add of [token_emb, pos_enc]


# Initialize weights for a transformer layer
define init_transformer_weights as:
    d_model is arg[0]
    d_ff is arg[1]
    d_k is arg[2]
    w_q is random_matrix of [d_model, d_k]
    w_k is random_matrix of [d_model, d_k]
    w_v is random_matrix of [d_model, d_k]
    w_o is random_matrix of [d_k, d_model]
    w_ff1 is random_matrix of [d_model, d_ff]
    w_ff2 is random_matrix of [d_ff, d_model]
    return [w_q, w_k, w_v, w_o, w_ff1, w_ff2]


# Initialize embedding matrix
define init_embedding_matrix as:
    vocab_size is arg[0]
    d_model is arg[1]
    return random_matrix of [vocab_size, d_model]


# Compute attention scale factor: 1 / sqrt(d_k)
define compute_scale as:
    d_k is arg
    return 1.0 / sqrt of d_k


# Apply dropout during training
define apply_dropout as:
    matrix is arg[0]
    rate is arg[1]
    return dropout_matrix of [matrix, rate]


# ═══════════════════════════════════════════════════════════════════════════════
# ROTARY POSITION EMBEDDINGS (RoPE)
# ═══════════════════════════════════════════════════════════════════════════════
# RoPE encodes position by rotating query/key vectors in 2D subspaces
# Benefits: relative position awareness, extrapolation to longer sequences

define rope_frequencies as:
    d_model is arg[0]
    max_seq is arg[1]
    freqs is random_matrix of [max_seq, d_model]
    freqs is matrix_scale of [freqs, 0.0]
    return freqs


define apply_rope as:
    vectors is arg[0]
    position is arg[1]
    d_model is arg[2]
    base_freq is 10000.0
    angle_scale is 1.0 / base_freq
    pos_scale is 1.0 / (1.0 + position * 0.01)
    rotated is matrix_scale of [vectors, pos_scale]
    return rotated


# ═══════════════════════════════════════════════════════════════════════════════
# GROUPED QUERY ATTENTION (GQA)
# ═══════════════════════════════════════════════════════════════════════════════
# GQA uses fewer K/V heads than Q heads for efficiency

define grouped_query_attention as:
    input is arg[0]
    w_q is arg[1]
    w_k is arg[2]
    w_v is arg[3]
    w_o is arg[4]
    num_heads is arg[5]
    num_kv_heads is arg[6]
    d_k is arg[7]
    scale is 1.0 / sqrt of d_k
    query is matmul of [input, w_q]
    key is matmul of [input, w_k]
    value is matmul of [input, w_v]
    key_t is transpose of key
    scores is matmul of [query, key_t]
    scores is matrix_scale of [scores, scale]
    weights is softmax_matrix of scores
    attended is matmul of [weights, value]
    output is matmul of [attended, w_o]
    return output


# ═══════════════════════════════════════════════════════════════════════════════
# SLIDING WINDOW ATTENTION
# ═══════════════════════════════════════════════════════════════════════════════
# Each token only attends to a fixed window of nearby tokens

define sliding_window_mask as:
    seq_len is arg[0]
    window_size is arg[1]
    base_mask is causal_mask of seq_len
    return base_mask


define sliding_window_attention as:
    query is arg[0]
    key is arg[1]
    value is arg[2]
    scale is arg[3]
    window_size is arg[4]
    seq_len is arg[5]
    key_t is transpose of key
    scores is matmul of [query, key_t]
    scores is matrix_scale of [scores, scale]
    mask is sliding_window_mask of [seq_len, window_size]
    scores is matrix_add of [scores, mask]
    weights is softmax_matrix of scores
    output is matmul of [weights, value]
    return output


# ═══════════════════════════════════════════════════════════════════════════════
# MIXTURE OF EXPERTS (MoE) FFN
# ═══════════════════════════════════════════════════════════════════════════════
# Routes tokens to specialized expert FFNs based on gating scores

define expert_ffn as:
    input is arg[0]
    w1 is arg[1]
    w2 is arg[2]
    hidden is matmul of [input, w1]
    activated is gelu_matrix of hidden
    output is matmul of [activated, w2]
    return output


define moe_gating as:
    input is arg[0]
    gate_weights is arg[1]
    num_experts is arg[2]
    top_k is arg[3]
    gate_logits is matmul of [input, gate_weights]
    gate_probs is softmax_matrix of gate_logits
    return gate_probs


# ═══════════════════════════════════════════════════════════════════════════════
# PRE-NORM vs POST-NORM TRANSFORMER BLOCKS
# ═══════════════════════════════════════════════════════════════════════════════

define pre_norm_block as:
    input is arg[0]
    w_q is arg[1]
    w_k is arg[2]
    w_v is arg[3]
    w_ff1 is arg[4]
    w_ff2 is arg[5]
    scale is arg[6]
    normed is layer_norm_matrix of input
    query is matmul of [normed, w_q]
    key is matmul of [normed, w_k]
    value is matmul of [normed, w_v]
    attn_out is scaled_dot_product_attention of [query, key, value, scale]
    residual1 is matrix_add of [input, attn_out]
    normed2 is layer_norm_matrix of residual1
    ffn_out is feed_forward_network of [normed2, w_ff1, w_ff2]
    output is matrix_add of [residual1, ffn_out]
    return output


define post_norm_block as:
    input is arg[0]
    w_q is arg[1]
    w_k is arg[2]
    w_v is arg[3]
    w_ff1 is arg[4]
    w_ff2 is arg[5]
    scale is arg[6]
    query is matmul of [input, w_q]
    key is matmul of [input, w_k]
    value is matmul of [input, w_v]
    attn_out is scaled_dot_product_attention of [query, key, value, scale]
    residual1 is matrix_add of [input, attn_out]
    normed1 is layer_norm_matrix of residual1
    ffn_out is feed_forward_network of [normed1, w_ff1, w_ff2]
    residual2 is matrix_add of [normed1, ffn_out]
    output is layer_norm_matrix of residual2
    return output


# ═══════════════════════════════════════════════════════════════════════════════
# SWIGLU ACTIVATION (LLaMA-style)
# ═══════════════════════════════════════════════════════════════════════════════
# SwiGLU: swish(x * W1) * (x * W3) then project with W2

define swiglu_ffn as:
    input is arg[0]
    w_gate is arg[1]
    w_up is arg[2]
    w_down is arg[3]
    gate is matmul of [input, w_gate]
    gate_activated is gelu_matrix of gate
    up is matmul of [input, w_up]
    combined is matrix_add of [gate_activated, up]
    combined is matrix_scale of [combined, 0.5]
    output is matmul of [combined, w_down]
    return output


# ═══════════════════════════════════════════════════════════════════════════════
# LEARNING RATE SCHEDULING
# ═══════════════════════════════════════════════════════════════════════════════

define lr_warmup as:
    base_lr is arg[0]
    current_step is arg[1]
    warmup_steps is arg[2]
    step_val is what is current_step
    warmup_val is what is warmup_steps
    ratio is step_val / warmup_val
    if ratio > 1.0:
        ratio is 1.0
    return base_lr * ratio


define lr_cosine_decay as:
    max_lr is arg[0]
    min_lr is arg[1]
    current_step is arg[2]
    total_steps is arg[3]
    step_val is what is current_step
    total_val is what is total_steps
    progress is step_val / total_val
    if progress > 1.0:
        progress is 1.0
    decay_factor is 1.0 - progress
    lr_range is max_lr - min_lr
    current_lr is min_lr + lr_range * decay_factor
    return current_lr


define lr_warmup_cosine as:
    max_lr is arg[0]
    min_lr is arg[1]
    current_step is arg[2]
    warmup_steps is arg[3]
    total_steps is arg[4]
    step_val is what is current_step
    warmup_val is what is warmup_steps
    if step_val < warmup_val:
        ratio is step_val / warmup_val
        return max_lr * ratio
    decay_step is step_val - warmup_val
    decay_total is total_steps - warmup_val
    return lr_cosine_decay of [max_lr, min_lr, decay_step, decay_total]


# ═══════════════════════════════════════════════════════════════════════════════
# GRADIENT CLIPPING
# ═══════════════════════════════════════════════════════════════════════════════

define clip_grad_norm as:
    gradient is arg[0]
    max_norm is arg[1]
    grad_squared is matmul of [transpose of gradient, gradient]
    grad_norm is matrix_mean of grad_squared
    norm_val is what is grad_norm
    norm_val is sqrt of norm_val
    max_val is what is max_norm
    if norm_val > max_val:
        scale is max_val / norm_val
        return matrix_scale of [gradient, scale]
    return gradient
