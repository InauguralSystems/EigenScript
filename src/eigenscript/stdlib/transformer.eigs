# Transformer Implementation for EigenScript
# A complete transformer architecture using EigenScript's matrix operations

# Core Attention Mechanism
# Scaled Dot-Product Attention: attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V
define scaled_dot_product_attention as:
    query is arg[0]
    key is arg[1]
    value is arg[2]
    scale is arg[3]
    key_t is transpose of key
    scores is matmul of [query, key_t]
    scaled_scores is matrix_scale of [scores, scale]
    weights is softmax_matrix of scaled_scores
    output is matmul of [weights, value]
    return output


# Masked Attention for decoder (causal masking)
define masked_attention as:
    query is arg[0]
    key is arg[1]
    value is arg[2]
    scale is arg[3]
    mask is arg[4]
    key_t is transpose of key
    scores is matmul of [query, key_t]
    scaled_scores is matrix_scale of [scores, scale]
    masked_scores is matrix_add of [scaled_scores, mask]
    weights is softmax_matrix of masked_scores
    output is matmul of [weights, value]
    return output


# Linear projection: output = input @ weights
define linear_projection as:
    input is arg[0]
    weights is arg[1]
    return matmul of [input, weights]


# Linear with bias: output = input @ weights + bias
define linear_with_bias as:
    input is arg[0]
    weights is arg[1]
    bias is arg[2]
    projected is matmul of [input, weights]
    return matrix_add of [projected, bias]


# Single attention head computation
define single_attention_head as:
    input is arg[0]
    w_q is arg[1]
    w_k is arg[2]
    w_v is arg[3]
    scale is arg[4]
    query is matmul of [input, w_q]
    key is matmul of [input, w_k]
    value is matmul of [input, w_v]
    return scaled_dot_product_attention of [query, key, value, scale]


# Multi-head attention (simplified for current EigenScript limitations)
define multi_head_attention as:
    input is arg[0]
    w_q is arg[1]
    w_k is arg[2]
    w_v is arg[3]
    w_o is arg[4]
    num_heads is arg[5]
    d_k is arg[6]
    scale is 1.0 / sqrt of d_k
    query is matmul of [input, w_q]
    key is matmul of [input, w_k]
    value is matmul of [input, w_v]
    attended is scaled_dot_product_attention of [query, key, value, scale]
    output is matmul of [attended, w_o]
    return output


# Position-wise Feed-Forward Network with GELU
define feed_forward_network as:
    input is arg[0]
    w1 is arg[1]
    w2 is arg[2]
    hidden is matmul of [input, w1]
    activated is gelu_matrix of hidden
    output is matmul of [activated, w2]
    return output


# Position-wise FFN with ReLU
define feed_forward_relu as:
    input is arg[0]
    w1 is arg[1]
    w2 is arg[2]
    hidden is matmul of [input, w1]
    activated is relu_matrix of hidden
    output is matmul of [activated, w2]
    return output


# Simplified Encoder Layer (single-head attention)
define encoder_layer_simple as:
    input is arg[0]
    w_q is arg[1]
    w_k is arg[2]
    w_v is arg[3]
    w_ff1 is arg[4]
    w_ff2 is arg[5]
    scale is arg[6]
    norm1 is layer_norm_matrix of input
    query is matmul of [norm1, w_q]
    key is matmul of [norm1, w_k]
    value is matmul of [norm1, w_v]
    attn_out is scaled_dot_product_attention of [query, key, value, scale]
    residual1 is matrix_add of [input, attn_out]
    norm2 is layer_norm_matrix of residual1
    ffn_hidden is matmul of [norm2, w_ff1]
    ffn_activated is gelu_matrix of ffn_hidden
    ffn_out is matmul of [ffn_activated, w_ff2]
    output is matrix_add of [residual1, ffn_out]
    return output


# Create token embeddings with positional encoding
define create_embeddings as:
    token_ids is arg[0]
    embed_matrix is arg[1]
    seq_len is arg[2]
    d_model is arg[3]
    token_emb is embedding_lookup of [embed_matrix, token_ids]
    pos_enc is sinusoidal_pe of [seq_len, d_model]
    return matrix_add of [token_emb, pos_enc]


# Initialize weights for a transformer layer
define init_transformer_weights as:
    d_model is arg[0]
    d_ff is arg[1]
    d_k is arg[2]
    w_q is random_matrix of [d_model, d_k]
    w_k is random_matrix of [d_model, d_k]
    w_v is random_matrix of [d_model, d_k]
    w_o is random_matrix of [d_k, d_model]
    w_ff1 is random_matrix of [d_model, d_ff]
    w_ff2 is random_matrix of [d_ff, d_model]
    return [w_q, w_k, w_v, w_o, w_ff1, w_ff2]


# Initialize embedding matrix
define init_embedding_matrix as:
    vocab_size is arg[0]
    d_model is arg[1]
    return random_matrix of [vocab_size, d_model]


# Compute attention scale factor: 1 / sqrt(d_k)
define compute_scale as:
    d_k is arg
    return 1.0 / sqrt of d_k


# Apply dropout during training
define apply_dropout as:
    matrix is arg[0]
    rate is arg[1]
    return dropout_matrix of [matrix, rate]


# ═══════════════════════════════════════════════════════════════════════════════
# ROTARY POSITION EMBEDDINGS (RoPE)
# ═══════════════════════════════════════════════════════════════════════════════
# RoPE encodes position by rotating query/key vectors in 2D subspaces
# Benefits: relative position awareness, extrapolation to longer sequences

define rope_frequencies as:
    d_model is arg[0]
    max_seq is arg[1]
    freqs is random_matrix of [max_seq, d_model]
    freqs is matrix_scale of [freqs, 0.0]
    return freqs


define apply_rope as:
    vectors is arg[0]
    position is arg[1]
    d_model is arg[2]
    base_freq is 10000.0
    angle_scale is 1.0 / base_freq
    pos_scale is 1.0 / (1.0 + position * 0.01)
    rotated is matrix_scale of [vectors, pos_scale]
    return rotated


# ═══════════════════════════════════════════════════════════════════════════════
# GROUPED QUERY ATTENTION (GQA)
# ═══════════════════════════════════════════════════════════════════════════════
# GQA uses fewer K/V heads than Q heads for efficiency

define grouped_query_attention as:
    input is arg[0]
    w_q is arg[1]
    w_k is arg[2]
    w_v is arg[3]
    w_o is arg[4]
    num_heads is arg[5]
    num_kv_heads is arg[6]
    d_k is arg[7]
    scale is 1.0 / sqrt of d_k
    query is matmul of [input, w_q]
    key is matmul of [input, w_k]
    value is matmul of [input, w_v]
    key_t is transpose of key
    scores is matmul of [query, key_t]
    scores is matrix_scale of [scores, scale]
    weights is softmax_matrix of scores
    attended is matmul of [weights, value]
    output is matmul of [attended, w_o]
    return output


# ═══════════════════════════════════════════════════════════════════════════════
# SLIDING WINDOW ATTENTION
# ═══════════════════════════════════════════════════════════════════════════════
# Each token only attends to a fixed window of nearby tokens

define sliding_window_mask as:
    seq_len is arg[0]
    window_size is arg[1]
    base_mask is causal_mask of seq_len
    return base_mask


define sliding_window_attention as:
    query is arg[0]
    key is arg[1]
    value is arg[2]
    scale is arg[3]
    window_size is arg[4]
    seq_len is arg[5]
    key_t is transpose of key
    scores is matmul of [query, key_t]
    scores is matrix_scale of [scores, scale]
    mask is sliding_window_mask of [seq_len, window_size]
    scores is matrix_add of [scores, mask]
    weights is softmax_matrix of scores
    output is matmul of [weights, value]
    return output


# ═══════════════════════════════════════════════════════════════════════════════
# MIXTURE OF EXPERTS (MoE) FFN
# ═══════════════════════════════════════════════════════════════════════════════
# Routes tokens to specialized expert FFNs based on gating scores

define expert_ffn as:
    input is arg[0]
    w1 is arg[1]
    w2 is arg[2]
    hidden is matmul of [input, w1]
    activated is gelu_matrix of hidden
    output is matmul of [activated, w2]
    return output


define moe_gating as:
    input is arg[0]
    gate_weights is arg[1]
    num_experts is arg[2]
    top_k is arg[3]
    gate_logits is matmul of [input, gate_weights]
    gate_probs is softmax_matrix of gate_logits
    return gate_probs


# ═══════════════════════════════════════════════════════════════════════════════
# PRE-NORM vs POST-NORM TRANSFORMER BLOCKS
# ═══════════════════════════════════════════════════════════════════════════════

define pre_norm_block as:
    input is arg[0]
    w_q is arg[1]
    w_k is arg[2]
    w_v is arg[3]
    w_ff1 is arg[4]
    w_ff2 is arg[5]
    scale is arg[6]
    normed is layer_norm_matrix of input
    query is matmul of [normed, w_q]
    key is matmul of [normed, w_k]
    value is matmul of [normed, w_v]
    attn_out is scaled_dot_product_attention of [query, key, value, scale]
    residual1 is matrix_add of [input, attn_out]
    normed2 is layer_norm_matrix of residual1
    ffn_out is feed_forward_network of [normed2, w_ff1, w_ff2]
    output is matrix_add of [residual1, ffn_out]
    return output


define post_norm_block as:
    input is arg[0]
    w_q is arg[1]
    w_k is arg[2]
    w_v is arg[3]
    w_ff1 is arg[4]
    w_ff2 is arg[5]
    scale is arg[6]
    query is matmul of [input, w_q]
    key is matmul of [input, w_k]
    value is matmul of [input, w_v]
    attn_out is scaled_dot_product_attention of [query, key, value, scale]
    residual1 is matrix_add of [input, attn_out]
    normed1 is layer_norm_matrix of residual1
    ffn_out is feed_forward_network of [normed1, w_ff1, w_ff2]
    residual2 is matrix_add of [normed1, ffn_out]
    output is layer_norm_matrix of residual2
    return output


# ═══════════════════════════════════════════════════════════════════════════════
# SWIGLU ACTIVATION (LLaMA-style)
# ═══════════════════════════════════════════════════════════════════════════════
# SwiGLU: swish(x * W1) * (x * W3) then project with W2

define swiglu_ffn as:
    input is arg[0]
    w_gate is arg[1]
    w_up is arg[2]
    w_down is arg[3]
    gate is matmul of [input, w_gate]
    gate_activated is gelu_matrix of gate
    up is matmul of [input, w_up]
    combined is matrix_add of [gate_activated, up]
    combined is matrix_scale of [combined, 0.5]
    output is matmul of [combined, w_down]
    return output


# ═══════════════════════════════════════════════════════════════════════════════
# LEARNING RATE SCHEDULING
# ═══════════════════════════════════════════════════════════════════════════════

define lr_warmup as:
    base_lr is arg[0]
    current_step is arg[1]
    warmup_steps is arg[2]
    step_val is what is current_step
    warmup_val is what is warmup_steps
    ratio is step_val / warmup_val
    if ratio > 1.0:
        ratio is 1.0
    return base_lr * ratio


define lr_cosine_decay as:
    max_lr is arg[0]
    min_lr is arg[1]
    current_step is arg[2]
    total_steps is arg[3]
    step_val is what is current_step
    total_val is what is total_steps
    progress is step_val / total_val
    if progress > 1.0:
        progress is 1.0
    decay_factor is 1.0 - progress
    lr_range is max_lr - min_lr
    current_lr is min_lr + lr_range * decay_factor
    return current_lr


define lr_warmup_cosine as:
    max_lr is arg[0]
    min_lr is arg[1]
    current_step is arg[2]
    warmup_steps is arg[3]
    total_steps is arg[4]
    step_val is what is current_step
    warmup_val is what is warmup_steps
    if step_val < warmup_val:
        ratio is step_val / warmup_val
        return max_lr * ratio
    decay_step is step_val - warmup_val
    decay_total is total_steps - warmup_val
    return lr_cosine_decay of [max_lr, min_lr, decay_step, decay_total]


# ═══════════════════════════════════════════════════════════════════════════════
# GRADIENT CLIPPING
# ═══════════════════════════════════════════════════════════════════════════════

define clip_grad_norm as:
    gradient is arg[0]
    max_norm is arg[1]
    grad_squared is matmul of [transpose of gradient, gradient]
    grad_norm is matrix_mean of grad_squared
    norm_val is what is grad_norm
    norm_val is sqrt of norm_val
    max_val is what is max_norm
    if norm_val > max_val:
        scale is max_val / norm_val
        return matrix_scale of [gradient, scale]
    return gradient


# ═══════════════════════════════════════════════════════════════════════════════
# CROSS-ENTROPY LOSS
# ═══════════════════════════════════════════════════════════════════════════════
# The standard loss function for language models
# CE(y, p) = -sum(y * log(p)) where y is one-hot target, p is prediction

define cross_entropy_loss as:
    logits is arg[0]
    target_idx is arg[1]
    probs is softmax_matrix of logits
    prob_list is matrix_to_list of probs
    row is prob_list[0]
    target_prob is row[target_idx]
    target_val is what is target_prob
    epsilon is 0.0000001
    safe_prob is target_val + epsilon
    neg_log_prob is 0.0 - log of safe_prob
    return neg_log_prob


define cross_entropy_with_label_smoothing as:
    logits is arg[0]
    target_idx is arg[1]
    smoothing is arg[2]
    vocab_size is arg[3]
    probs is softmax_matrix of logits
    prob_list is matrix_to_list of probs
    row is prob_list[0]
    target_prob is row[target_idx]
    target_val is what is target_prob
    epsilon is 0.0000001
    smooth_val is what is smoothing
    vocab_val is what is vocab_size
    confidence is 1.0 - smooth_val
    smooth_dist is smooth_val / vocab_val
    adjusted_target is confidence * target_val + smooth_dist
    safe_prob is adjusted_target + epsilon
    neg_log_prob is 0.0 - log of safe_prob
    return neg_log_prob


# ═══════════════════════════════════════════════════════════════════════════════
# PERPLEXITY
# ═══════════════════════════════════════════════════════════════════════════════
# PPL = exp(average cross-entropy loss)
# Lower is better; measures how "surprised" the model is

define perplexity_from_loss as:
    avg_loss is arg[0]
    loss_val is what is avg_loss
    ppl is exp of loss_val
    return ppl


# ═══════════════════════════════════════════════════════════════════════════════
# ADAMW OPTIMIZER
# ═══════════════════════════════════════════════════════════════════════════════
# AdamW: Adam with decoupled weight decay (standard for LLMs)
# Unlike Adam, weight decay is applied directly to weights, not through gradient

define adamw_step as:
    weight is arg[0]
    gradient is arg[1]
    m_state is arg[2]
    v_state is arg[3]
    lr is arg[4]
    beta1 is arg[5]
    beta2 is arg[6]
    epsilon is arg[7]
    weight_decay is arg[8]
    step_num is arg[9]
    beta1_val is what is beta1
    beta2_val is what is beta2
    step_val is what is step_num
    one_minus_b1 is 1.0 - beta1_val
    one_minus_b2 is 1.0 - beta2_val
    new_m is matrix_scale of [m_state, beta1_val]
    grad_contrib is matrix_scale of [gradient, one_minus_b1]
    new_m is matrix_add of [new_m, grad_contrib]
    new_v is matrix_scale of [v_state, beta2_val]
    grad_sq is matmul of [transpose of gradient, gradient]
    grad_sq_scaled is matrix_scale of [grad_sq, one_minus_b2]
    bias_correct1 is 1.0 - (beta1_val * beta1_val)
    bias_correct2 is 1.0 - (beta2_val * beta2_val)
    lr_val is what is lr
    wd_val is what is weight_decay
    decay_term is matrix_scale of [weight, wd_val]
    weight_after_decay is matrix_add of [weight, matrix_scale of [decay_term, -1.0]]
    update is matrix_scale of [new_m, lr_val]
    new_weight is matrix_add of [weight_after_decay, matrix_scale of [update, -1.0]]
    return [new_weight, new_m, new_v]


# ═══════════════════════════════════════════════════════════════════════════════
# CROSS-ATTENTION (for encoder-decoder models)
# ═══════════════════════════════════════════════════════════════════════════════
# Query from decoder, Key/Value from encoder

define cross_attention as:
    decoder_hidden is arg[0]
    encoder_output is arg[1]
    w_q is arg[2]
    w_k is arg[3]
    w_v is arg[4]
    scale is arg[5]
    query is matmul of [decoder_hidden, w_q]
    key is matmul of [encoder_output, w_k]
    value is matmul of [encoder_output, w_v]
    key_t is transpose of key
    scores is matmul of [query, key_t]
    scores is matrix_scale of [scores, scale]
    weights is softmax_matrix of scores
    output is matmul of [weights, value]
    return output


# ═══════════════════════════════════════════════════════════════════════════════
# KV CACHE (reusable)
# ═══════════════════════════════════════════════════════════════════════════════
# Caches key/value for efficient autoregressive generation

define kv_cache_append as:
    cache is arg[0]
    new_kv is arg[1]
    use_cache is arg[2]
    cache_flag is what is use_cache
    if cache_flag > 0:
        updated is matrix_concat of [cache, new_kv]
        return updated
    return new_kv


define cached_attention as:
    new_q is arg[0]
    new_k is arg[1]
    new_v is arg[2]
    k_cache is arg[3]
    v_cache is arg[4]
    scale is arg[5]
    use_cache is arg[6]
    full_k is kv_cache_append of [k_cache, new_k, use_cache]
    full_v is kv_cache_append of [v_cache, new_v, use_cache]
    key_t is transpose of full_k
    scores is matmul of [new_q, key_t]
    scores is matrix_scale of [scores, scale]
    weights is softmax_matrix of scores
    attended is matmul of [weights, full_v]
    return [attended, full_k, full_v]


# ═══════════════════════════════════════════════════════════════════════════════
# WEIGHT INITIALIZATION OPTIONS
# ═══════════════════════════════════════════════════════════════════════════════

define he_init as:
    shape is arg[0]
    fan_in is shape[0]
    fan_in_val is what is fan_in
    std is sqrt of (2.0 / fan_in_val)
    weights is random_matrix of shape
    weights is matrix_scale of [weights, std]
    return weights


define orthogonal_init as:
    shape is arg[0]
    weights is random_matrix of shape
    weights is layer_norm_matrix of weights
    return weights


define normal_init as:
    shape is arg[0]
    std is arg[1]
    weights is random_matrix of shape
    std_val is what is std
    weights is matrix_scale of [weights, std_val]
    return weights


# ═══════════════════════════════════════════════════════════════════════════════
# DECODER BLOCK (full transformer decoder layer)
# ═══════════════════════════════════════════════════════════════════════════════
# Self-attention + optional cross-attention + FFN

define decoder_block as:
    input is arg[0]
    encoder_out is arg[1]
    w_self_q is arg[2]
    w_self_k is arg[3]
    w_self_v is arg[4]
    w_cross_q is arg[5]
    w_cross_k is arg[6]
    w_cross_v is arg[7]
    w_ff1 is arg[8]
    w_ff2 is arg[9]
    scale is arg[10]
    seq_len is arg[11]
    normed1 is layer_norm_matrix of input
    self_q is matmul of [normed1, w_self_q]
    self_k is matmul of [normed1, w_self_k]
    self_v is matmul of [normed1, w_self_v]
    self_attn is masked_attention of [self_q, self_k, self_v, scale, causal_mask of seq_len]
    residual1 is matrix_add of [input, self_attn]
    normed2 is layer_norm_matrix of residual1
    cross_out is cross_attention of [normed2, encoder_out, w_cross_q, w_cross_k, w_cross_v, scale]
    residual2 is matrix_add of [residual1, cross_out]
    normed3 is layer_norm_matrix of residual2
    ffn_out is feed_forward_network of [normed3, w_ff1, w_ff2]
    output is matrix_add of [residual2, ffn_out]
    return output


# ═══════════════════════════════════════════════════════════════════════════════
# TEMPERATURE SAMPLING UTILITIES
# ═══════════════════════════════════════════════════════════════════════════════

define apply_temperature as:
    logits is arg[0]
    temperature is arg[1]
    temp_val is what is temperature
    inv_temp is 1.0 / temp_val
    scaled is matrix_scale of [logits, inv_temp]
    return scaled


define apply_repetition_penalty as:
    logits is arg[0]
    generated_tokens is arg[1]
    penalty is arg[2]
    penalty_val is what is penalty
    token_count is len of generated_tokens
    count_val is what is token_count
    scale_factor is 1.0 - (count_val * 0.01 * penalty_val)
    if scale_factor < 0.5:
        scale_factor is 0.5
    penalized is matrix_scale of [logits, scale_factor]
    return penalized


# ═══════════════════════════════════════════════════════════════════════════════
# SEQUENCE UTILITIES
# ═══════════════════════════════════════════════════════════════════════════════

define pad_sequence as:
    tokens is arg[0]
    target_len is arg[1]
    pad_token is arg[2]
    current_len is len of tokens
    curr_val is what is current_len
    target_val is what is target_len
    padded is tokens
    pad_idx is curr_val
    loop while pad_idx < target_val:
        pad_val is what is pad_token
        _d is append of [padded, pad_val]
        pad_idx is pad_idx + 1
    return padded


define truncate_sequence as:
    tokens is arg[0]
    max_len is arg[1]
    current_len is len of tokens
    curr_val is what is current_len
    max_val is what is max_len
    if curr_val <= max_val:
        return tokens
    truncated is []
    idx is 0
    loop while idx < max_val:
        tok is tokens[idx]
        tok_val is what is tok
        _d is append of [truncated, tok_val]
        idx is idx + 1
    return truncated
